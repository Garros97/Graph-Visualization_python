{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brazil town city', 'brazil history', 'brazil history', 'brazil culture video', 'brazil culture video', 'Brasilia', 'Brasilia congress', 'Brasilia congress is located', 'Brasilia constitutions', 'Brasilia constitutions how many', 'Brasilia twin towns', 'Brasilia transportation', 'Brasilia transportation underground', 'Brasilia transport ticket', 'Brasilia transport ticket', 'Brasilia transport ticket', 'Brasilia temple', 'Berlin boroughs', 'Berlin borough', 'Berlin borough how many', 'Berlin airport', 'Berlin airports', 'Berlin museum land', 'Berlin street art', 'Berlin wall', 'Berlin protestant', 'berlin airport', 'how many airports does berlin have', 'berlin borough', 'berlin boroughs', 'berlin borough', 'Hauptbahnhof', 'berlin wall graffiti', 'fall of berlin wall', 'berlin sister city', 'berlin museum island', 'berlin wall graffiti which side', 'berlin wall graffiti', 'berlin Westhafen', 'berlin seat executive', 'beijing airport', 'beijing airport', 'beijing airports', 'beijing history', 'beijing population', 'beijing religion', 'beijing atheist', 'Chinese traditional religion', 'beijing religion', 'beijing religion christian', 'beijing summer palace', 'beijing transportation', 'Blue of Jingtai', 'bejiing art', 'bejiing politics', 'most populous cities', 'capital of china', 'districts of beijing', 'china cso', 'china sco', 'shanghai cooperation organization', 'china cities population', 'forbidden city', 'china railways', 'beijing subway', 'airports in beijing', 'Blue of Jingtai', 'beijing demographics', 'beijing religion']\n",
      "\n",
      "\n",
      "\n",
      "['subway', 'Blue', 'religion', 'culture', 'forbidden', 'atheist', 'island', 'town', 'borough', 'of', 'street', 'beijing', 'museum', 'organization', 'is', 'shanghai', 'art', 'Brasilia', 'congress', 'constitutions', 'airport', 'land', 'city', 'history', 'transport', 'protestant', 'which', 'Chinese', 'traditional', 'christian', 'summer', 'politics', 'populous', 'cso', 'how', 'bejiing', 'have', 'fall', 'underground', 'districts', 'Berlin', 'railways', 'towns', 'ticket', 'demographics', 'Jingtai', 'palace', 'in', 'wall', 'executive', 'boroughs', 'population', 'located', 'temple', 'seat', 'sco', 'brazil', 'cooperation', 'does', 'Hauptbahnhof', 'side', 'capital', 'airports', 'graffiti', 'most', 'video', 'berlin', 'china', 'many', 'twin', 'cities', 'sister', 'transportation', 'Westhafen']\n"
     ]
    }
   ],
   "source": [
    "# extract and clean the data\n",
    "liste = pd.read_excel ('List.xlsx')\n",
    "sentences_list = list(liste[\"music download\"])\n",
    "token_list = list(set(\" \".join(sentences_list).split(\" \")))\n",
    "print(sentences_list)\n",
    "print(\"\\n\\n\")\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': ['town', 'history', 'history', 'culture', 'culture'],\n",
       " 'town': ['brazil'],\n",
       " 'city': ['town', 'sister', 'forbidden'],\n",
       " 'history': ['brazil', 'brazil', 'beijing'],\n",
       " 'culture': ['brazil', 'video', 'brazil'],\n",
       " 'video': ['culture', 'culture'],\n",
       " 'brasilia': ['congress',\n",
       "  'congress',\n",
       "  'constitutions',\n",
       "  'constitutions',\n",
       "  'twin',\n",
       "  'transportation',\n",
       "  'transportation',\n",
       "  'transport',\n",
       "  'transport',\n",
       "  'transport',\n",
       "  'temple'],\n",
       " 'congress': ['brasilia', 'is', 'brasilia'],\n",
       " 'is': ['congress'],\n",
       " 'located': ['is'],\n",
       " 'constitutions': ['brasilia', 'how', 'brasilia'],\n",
       " 'how': ['constitutions', 'many', 'borough', 'many'],\n",
       " 'many': ['how', 'how', 'airports', 'how'],\n",
       " 'twin': ['brasilia'],\n",
       " 'towns': ['twin'],\n",
       " 'transportation': ['brasilia', 'underground', 'brasilia', 'beijing'],\n",
       " 'underground': ['transportation'],\n",
       " 'transport': ['brasilia', 'ticket', 'brasilia', 'ticket', 'brasilia'],\n",
       " 'ticket': ['transport', 'transport', 'transport'],\n",
       " 'temple': ['brasilia'],\n",
       " 'berlin': ['boroughs',\n",
       "  'borough',\n",
       "  'borough',\n",
       "  'airport',\n",
       "  'airports',\n",
       "  'museum',\n",
       "  'street',\n",
       "  'wall',\n",
       "  'protestant',\n",
       "  'airport',\n",
       "  'have',\n",
       "  'does',\n",
       "  'borough',\n",
       "  'boroughs',\n",
       "  'borough',\n",
       "  'wall',\n",
       "  'wall',\n",
       "  'of',\n",
       "  'sister',\n",
       "  'museum',\n",
       "  'wall',\n",
       "  'wall',\n",
       "  'westhafen',\n",
       "  'seat'],\n",
       " 'boroughs': ['berlin', 'berlin'],\n",
       " 'borough': ['berlin', 'how', 'berlin', 'berlin', 'berlin'],\n",
       " 'airport': ['berlin', 'berlin', 'beijing', 'beijing'],\n",
       " 'airports': ['berlin', 'does', 'many', 'beijing', 'in'],\n",
       " 'museum': ['berlin', 'island', 'berlin'],\n",
       " 'land': ['museum'],\n",
       " 'street': ['berlin'],\n",
       " 'art': ['street', 'bejiing'],\n",
       " 'wall': ['berlin',\n",
       "  'graffiti',\n",
       "  'berlin',\n",
       "  'berlin',\n",
       "  'graffiti',\n",
       "  'berlin',\n",
       "  'graffiti',\n",
       "  'berlin'],\n",
       " 'protestant': ['berlin'],\n",
       " 'does': ['airports'],\n",
       " 'have': ['berlin'],\n",
       " 'hauptbahnhof': [],\n",
       " 'graffiti': ['wall', 'which', 'wall', 'wall'],\n",
       " 'fall': ['of'],\n",
       " 'of': ['fall',\n",
       "  'jingtai',\n",
       "  'blue',\n",
       "  'china',\n",
       "  'capital',\n",
       "  'beijing',\n",
       "  'districts',\n",
       "  'jingtai',\n",
       "  'blue'],\n",
       " 'sister': ['berlin'],\n",
       " 'island': ['museum'],\n",
       " 'which': ['graffiti'],\n",
       " 'side': ['which'],\n",
       " 'westhafen': ['berlin'],\n",
       " 'seat': ['berlin'],\n",
       " 'executive': ['seat'],\n",
       " 'beijing': ['airport',\n",
       "  'airport',\n",
       "  'airports',\n",
       "  'history',\n",
       "  'population',\n",
       "  'religion',\n",
       "  'atheist',\n",
       "  'religion',\n",
       "  'religion',\n",
       "  'summer',\n",
       "  'transportation',\n",
       "  'of',\n",
       "  'subway',\n",
       "  'in',\n",
       "  'demographics',\n",
       "  'religion'],\n",
       " 'population': ['beijing', 'cities'],\n",
       " 'religion': ['beijing',\n",
       "  'traditional',\n",
       "  'beijing',\n",
       "  'christian',\n",
       "  'beijing',\n",
       "  'beijing'],\n",
       " 'atheist': ['beijing'],\n",
       " 'chinese': ['traditional'],\n",
       " 'traditional': ['chinese'],\n",
       " 'christian': ['religion'],\n",
       " 'summer': ['beijing'],\n",
       " 'palace': ['summer'],\n",
       " 'blue': ['of', 'of'],\n",
       " 'jingtai': ['of', 'of'],\n",
       " 'bejiing': ['art', 'politics'],\n",
       " 'politics': ['bejiing'],\n",
       " 'most': ['populous'],\n",
       " 'populous': ['most'],\n",
       " 'cities': ['populous', 'population', 'china'],\n",
       " 'capital': ['of'],\n",
       " 'china': ['of', 'cso', 'sco', 'cities', 'railways'],\n",
       " 'districts': ['of'],\n",
       " 'cso': ['china'],\n",
       " 'sco': ['china'],\n",
       " 'shanghai': ['cooperation'],\n",
       " 'cooperation': ['shanghai'],\n",
       " 'organization': ['cooperation'],\n",
       " 'forbidden': ['city'],\n",
       " 'railways': ['china'],\n",
       " 'subway': ['beijing'],\n",
       " 'in': ['airports'],\n",
       " 'demographics': ['beijing']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Representation of the whole graph\n",
    "def extract_graph(datas):\n",
    "# dictionary {keys=word, value= neighbor}\n",
    "    graph = {}\n",
    "    # split the words\n",
    "    for data in datas:\n",
    "        words = data.split(\" \")\n",
    "        # if the word is alone other if the length of the word is 1\n",
    "        if len(words) == 1:\n",
    "            if words[0].lower() in graph.keys():\n",
    "                pass\n",
    "            else:\n",
    "                graph[words[0].lower()] = []\n",
    "        else:\n",
    "            for i in range(len(words)):\n",
    "                # if the word is at the position 0\n",
    "                if i == 0:\n",
    "                    if words[i].lower() in graph.keys():\n",
    "                        graph[words[i].lower()].append(words[i+1].lower())\n",
    "                    else:\n",
    "                         graph[words[i].lower()] = [words[i+1].lower()]\n",
    "                # if the word is at the end position \n",
    "                elif i == len(words) - 1:\n",
    "                    if words[i].lower() in graph.keys():\n",
    "                        graph[words[i].lower()].append(words[i-1].lower())\n",
    "                    else:\n",
    "                         graph[words[i].lower()] = [words[i-1].lower()]\n",
    "                # if the word is at the second last position\n",
    "                else:\n",
    "                    if words[i].lower() in graph.keys():\n",
    "                        graph[words[i].lower()].append(words[i+1].lower())\n",
    "                        graph[words[i].lower()].append(words[i-1].lower())\n",
    "                    else:\n",
    "                        graph[words[i].lower()] = [words[i+1].lower()]\n",
    "                        graph[words[i].lower()] = [words[i-1].lower()]\n",
    "    #remove the duplicate(words, which come <1 time )\n",
    "    #for key in graph.keys():\n",
    "    #    graph[key] = list(set(graph[key]))\n",
    "    return graph\n",
    "g = extract_graph(sentences_list)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def find_categories(tokens, num_cat):\n",
    "    text = nltk.word_tokenize(\" \".join(tokens))\n",
    "    token_cat = nltk.pos_tag(text)\n",
    "    cats = [a[1] for a in token_cat]\n",
    "    cats_count = collections.Counter(x for x in cats)\n",
    "    \n",
    "    return list(cats_count.keys())[:num_cat]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of the reduced graph\n",
    "def extract_graph_for_js(graph, model):\n",
    "    nodes = []\n",
    "    links = []\n",
    "    #print(nodes)\n",
    "    i = 1\n",
    "    all_node = []\n",
    "    for key in graph.keys():\n",
    "        all_node.append(model.predict(tfidf_vectorizer.transform([key]))[0])\n",
    "        #for j in range(len(group_5.keys())):\n",
    "            #if key in group_5[list(group_5.keys())[j]]:\n",
    "                #group = j +1\n",
    "                #break\n",
    "    for node in set(all_node):\n",
    "        nodes.append({\n",
    "            \"id\": str(node),\n",
    "            \"group\": str(node)\n",
    "        })\n",
    "        \n",
    "    for key in graph.keys():\n",
    "        s_name = str(model.predict(tfidf_vectorizer.transform([key]))[0])\n",
    "        for word in graph[key]:\n",
    "            d_name = str(model.predict(tfidf_vectorizer.transform([word]))[0])\n",
    "            exist = False\n",
    "            for obj in links:\n",
    "                if obj[\"source\"] == s_name and obj[\"target\"]==d_name:\n",
    "                    exist = True\n",
    "                    obj[\"value\"] +=1\n",
    "                    break\n",
    "            else:\n",
    "                links.append({\n",
    "                    \"source\": s_name,\n",
    "                    \"target\": d_name,\n",
    "                    \"value\": 1\n",
    "                })\n",
    "\n",
    "    json_dict = {\"nodes\": nodes, \"links\": links}\n",
    "    \n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnltk.download(\\'punkt\\')\\nnltk.download(\\'averaged_perceptron_tagger\\')\\ntext = nltk.word_tokenize(\" \".join(token_list))\\ntoken_cat = nltk.pos_tag(text)\\ncats = [a[1] for a in token_cat]\\nprint(cats)\\nprint(len(set(cats)))\\n '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "text = nltk.word_tokenize(\" \".join(token_list))\n",
    "token_cat = nltk.pos_tag(text)\n",
    "cats = [a[1] for a in token_cat]\n",
    "print(cats)\n",
    "print(len(set(cats)))\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_categories(token_list, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.pos_tag([\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 6, 3, 3, 0, 0, 0, 2, 4, 0, 6, 2, 0, 0, 0, 0, 1, 1, 1, 2, 0,\n",
       "       3, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 4, 2, 0, 0, 1,\n",
       "       0, 4, 0, 0, 7, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 7, 0, 3,\n",
       "       2, 0, 2, 0, 0, 3, 5, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "# import string library function \n",
    "import string\n",
    "\n",
    "set_tokens = set(token_list)\n",
    "\n",
    "def compute_tf(word_dict, l):\n",
    "    tf = {}\n",
    "    sum_nk = len(l)\n",
    "    for word, count in word_dict.items():\n",
    "        tf[word] = count/sum_nk\n",
    "    return tf\n",
    "\n",
    "def compute_idf(strings_list):\n",
    "    n = len(strings_list)\n",
    "    idf = dict.fromkeys(strings_list[0].keys(), 0)\n",
    "    for l in strings_list:\n",
    "        for word, count in l.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    for word, v in idf.items():\n",
    "        idf[word] = log(n / float(v))\n",
    "    return idf\n",
    "\n",
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"[{}]\".format(string.punctuation), \" \", line)\n",
    "    return line\n",
    "    \n",
    "#idf = compute_idf([word_dict_A, word_dict_B, word_dict_C])\n",
    "#all_text = \". \".join(sentences_list)\n",
    "#print(all_text)\n",
    "#input()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessing)\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences_list)\n",
    "\n",
    "kmeans = KMeans(n_clusters=8).fit(tfidf)\n",
    "\n",
    "#lines_for_predicting = [\"berlin\", \"china\", \"religion\", \"town\", \"history\", \"culture\", \"video\"]\n",
    "kmeans.predict(tfidf_vectorizer.transform(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "graph = extract_graph(sentences_list)\n",
    "\n",
    "nodes = [5, 8, 11]\n",
    "for node in nodes:\n",
    "    kmeans = KMeans(n_clusters=node).fit(tfidf)\n",
    "    data = extract_graph_for_js(graph, kmeans)\n",
    "    filename = f\"graph_with_{node}_node.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
